{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import pandas as pd\n","\n","\n","data_file = \"sen.csv\"\n","\n","base_df = pd.read_csv(\"../Data/\" + data_file, parse_dates=[\"date\"])\n","congress = pd.read_csv(\"../Data/legislators-current.csv\")\n","\n","# Assume independents (Bernie and Angus King) effectively align with Democrats\n","congress.loc[(congress.party == \"Independent\"), \"party\"] = \"Democrat/Ind\"\n","congress.loc[(congress.party == \"Democrat\"), \"party\"] = \"Democrat/Ind\"\n","\n","# Add outcome (democrat/republican)\n","base_df = base_df.merge(\n","    congress[[\"twitter\", \"party\"]], how=\"left\", left_on=\"username\", right_on=\"twitter\"\n",")\n","\n","# Filter unnecessary columns\n","keep_cols = [\"username\", \"to\", \"text\", \"date\", \"hashtags\", \"mentions\", \"urls\", \"party\"]\n","base_df = base_df[keep_cols]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import string\n","from urllib.parse import urlparse\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","# Remove noisy urls\n","def remove_all_urls(text, urls):\n","    if str(urls) == \"\":\n","        return str(text).strip()\n","    elif pd.isnull(text):\n","        return \"\"\n","    else:\n","        url_list = str(urls).split(sep=\",\")\n","        for i in url_list:\n","            text = str(text).replace(i, \"\")\n","        return str(text).strip()\n","\n","\n","base_df[\"no_url_text\"] = base_df.apply(\n","    lambda x: remove_all_urls(x[\"text\"], x[\"urls\"]), axis=1\n",")\n","\n","# Define function to strip url to base\n","def clean_urls(urls):\n","    if str(urls) == \"\":\n","        return \"\"\n","    else:\n","        parse_list = list(map(urlparse, str(urls).split(sep=\",\")))\n","        url_list = [i[1] for i in parse_list]\n","        return \" \".join(url_list)\n","\n","\n","# Strip urls\n","base_df[\"clean_urls\"] = base_df[\"urls\"].apply(clean_urls)\n","\n","# Recombine base urls with text\n","base_df[\"clean_text\"] = (\n","    base_df[\"no_url_text\"].astype(str) + \" \" + base_df[\"clean_urls\"].astype(str)\n",")\n","\n","custom_punctuation1 = string.punctuation.replace(\"@\", \"\").replace(\"#\", \"\")\n","\n","# Define function to count all caps words\n","def all_caps(text):\n","    if str(text) == \"\":\n","        return 0\n","    else:\n","        caps_list = str(text).upper().split()\n","        count = 0\n","        for w in list(set(caps_list)):\n","            if w.strip(custom_punctuation1).isalpha() & (\n","                len(w.strip(custom_punctuation1)) > 1\n","            ):\n","                count = count + (\" \" + text + \" \").count(\" \" + w + \" \")\n","        return count\n","\n","\n","base_df[\"all_caps\"] = base_df[\"clean_text\"].apply(all_caps)\n","\n","# Define function to count capitalized words\n","def cap_words(text):\n","    if str(text) == \"\":\n","        return 0\n","    else:\n","        caps_list = list(map(str.capitalize, str(text).split()))\n","        count = 0\n","        for w in list(set(caps_list)):\n","            if w.strip(custom_punctuation1).isalpha():\n","                count = count + (\" \" + text + \" \").count(\" \" + w + \" \")\n","        return count\n","\n","\n","base_df[\"cap_words\"] = base_df[\"clean_text\"].apply(cap_words)\n","\n","# Basic Sentiment Score\n","analyzer = SentimentIntensityAnalyzer()\n","\n","\n","def compound(text):\n","    return analyzer.polarity_scores(text)[\"compound\"]\n","\n","\n","base_df[\"sentiment_compound\"] = base_df[\"no_url_text\"].apply(compound)\n","\n","\n","def neg(text):\n","    return analyzer.polarity_scores(text)[\"neg\"]\n","\n","\n","base_df[\"sentiment_neg\"] = base_df[\"no_url_text\"].apply(neg)\n","\n","\n","def pos(text):\n","    return analyzer.polarity_scores(text)[\"pos\"]\n","\n","\n","base_df[\"sentiment_pos\"] = base_df[\"no_url_text\"].apply(pos)\n","\n","\n","def neu(text):\n","    return analyzer.polarity_scores(text)[\"neu\"]\n","\n","\n","base_df[\"sentiment_neu\"] = base_df[\"no_url_text\"].apply(neu)\n","\n","# Drop tweets with no text or url data\n","ml_df = base_df[base_df[\"text\"].notna() | base_df[\"urls\"].notna()]\n","dropped = base_df[base_df[\"text\"].isna() & base_df[\"urls\"].isna()]\n","\n","# df for base model\n","simple_df = base_df[base_df[\"text\"].notna()]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Let's try spacy\n","import en_core_web_sm\n","from sklearn.base import TransformerMixin\n","from sklearn.pipeline import Pipeline\n","import string\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from spacy.lang.en import English\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import metrics\n","\n","nlp = en_core_web_sm.load()\n","\n","# Punctuation marks\n","punctuations = string.punctuation\n","custom_punctuation2 = punctuations.replace(\"@\", \"\").replace(\"!\", \"\").replace(\"#\", \"\")\n","\n","# Add nan to STOP_WORDS\n","# STOP_WORDS\n","\n","# Load English tokenizer, tagger, parser, NER and word vectors\n","parser = English()\n","\n","# Creating our tokenizer function\n","def spacy_tokenizer(sentence):\n","    # Creating our token object, which is used to create documents with linguistic annotations.\n","    mytokens = parser(sentence)\n","\n","    # Lemmatizing each token and converting each token into lowercase\n","    mytokens = [\n","        word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_\n","        for word in mytokens\n","    ]\n","\n","    # Removing stop words\n","    mytokens = [\n","        word for word in mytokens if word not in STOP_WORDS and word not in punctuations\n","    ]\n","\n","    # return preprocessed list of tokens\n","    return mytokens\n","\n","\n","# Custom transformer using spaCy\n","class predictors(TransformerMixin):\n","    def transform(self, X, **transform_params):\n","        # Cleaning Text\n","        return [clean_text(text) for text in X]\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","\n","    def get_params(self, deep=True):\n","        return {}\n","\n","\n","# Basic function to clean the text\n","def clean_text(text):\n","    return text.strip().lower()\n","\n","\n","# Create spacy vectorizers\n","spacy_cv1 = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 1))\n","spacy_tf1 = TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 1))\n","spacy_cv2 = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 2))\n","spacy_tf2 = TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 2))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier = RandomForestClassifier()\n","\n","# Make train test splits\n","x = ml_df[\"clean_text\"]\n","y = ml_df[\"party\"]\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n","\n","# Create pipeline\n","pipe = Pipeline(\n","    [(\"cleaner\", predictors()), (\"vectorizer\", spacy_cv1), (\"classifier\", classifier)]\n",")\n","\n","# model generation\n","pipe.fit(x_train, y_train)\n","\n","# Predicting with a test dataset\n","predicted = pipe.predict(x_test)\n","\n","# Model Accuracy\n","print(\"Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","text_counts = spacy_cv1.fit_transform(ml_df[\"clean_text\"])\n","feature_names = spacy_cv1.get_feature_names()\n","\n","\n","def spacy_tokenizer2(sentence):\n","    # Creating our token object, which is used to create documents with linguistic annotations.\n","    mytokens = parser(sentence)\n","\n","    # Lemmatizing each token and converting each token into lowercase\n","    mytokens = [\n","        word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_\n","        for word in mytokens\n","    ]\n","\n","    # Removing stop words\n","    mytokens = [\n","        word for word in mytokens if word not in STOP_WORDS and word not in punctuations\n","    ]\n","\n","    # return preprocessed list of tokens\n","    return mytokens\n","\n","\n","sentence = \"Donald Trump visited Apple headquarters THIS IS a smaple as;odfi n soid lkloiw fj;aklsjfd -sdiof @cnn #jfs\"\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}