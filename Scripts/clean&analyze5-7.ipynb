{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import pandas as pd\n","\n","\n","data_file = \"sen.csv\"\n","\n","base_df = pd.read_csv(\"../Data/\" + data_file, parse_dates=[\"date\"])\n","congress = pd.read_csv(\"../Data/legislators-current.csv\")\n","\n","# Assume independents (Bernie and Angus King) effectively align with Democrats\n","congress.loc[(congress.party == \"Independent\"), \"party\"] = \"Democrat/Ind\"\n","congress.loc[(congress.party == \"Democrat\"), \"party\"] = \"Democrat/Ind\"\n","\n","# Add outcome (democrat/republican)\n","base_df = base_df.merge(\n","    congress[[\"twitter\", \"party\"]], how=\"left\", left_on=\"username\", right_on=\"twitter\"\n",")\n","\n","# Filter unnecessary columns\n","keep_cols = [\"username\", \"to\", \"text\", \"date\", \"hashtags\", \"mentions\", \"urls\", \"party\"]\n","base_df = base_df[keep_cols]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import string\n","from urllib.parse import urlparse\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import numpy as np\n","\n","# Numeric Date\n","base_df[\"num_days\"] = (base_df[\"date\"] - base_df[\"date\"].min()) / np.timedelta64(1, \"D\")\n","\n","# Remove noisy urls\n","def remove_all_urls(text, urls):\n","    if str(urls) == \"\":\n","        return str(text).strip()\n","    elif pd.isnull(text):\n","        return \"\"\n","    else:\n","        url_list = str(urls).split(sep=\",\")\n","        for i in url_list:\n","            text = str(text).replace(i, \"\")\n","        return str(text).strip()\n","\n","\n","base_df[\"no_url_text\"] = base_df.apply(\n","    lambda x: remove_all_urls(x[\"text\"], x[\"urls\"]), axis=1\n",")\n","\n","# Define function to strip url to base\n","def clean_urls(urls):\n","    if str(urls) == \"\":\n","        return \"\"\n","    else:\n","        parse_list = list(map(urlparse, str(urls).split(sep=\",\")))\n","        url_list = [i[1] for i in parse_list]\n","        return \" \".join(url_list)\n","\n","\n","# Strip urls\n","base_df[\"clean_urls\"] = base_df[\"urls\"].apply(clean_urls)\n","\n","# Recombine base urls with text\n","base_df[\"clean_text\"] = (\n","    base_df[\"no_url_text\"].astype(str) + \" \" + base_df[\"clean_urls\"].astype(str)\n",")\n","\n","custom_punctuation1 = string.punctuation.replace(\"@\", \"\").replace(\"#\", \"\")\n","\n","# Define function to count all caps words\n","def all_caps(text):\n","    if str(text) == \"\":\n","        return 0\n","    else:\n","        caps_list = str(text).upper().split()\n","        count = 0\n","        for w in list(set(caps_list)):\n","            if w.strip(custom_punctuation1).isalpha() & (\n","                len(w.strip(custom_punctuation1)) > 1\n","            ):\n","                count = count + (\" \" + text + \" \").count(\" \" + w + \" \")\n","        return count\n","\n","\n","base_df[\"all_caps\"] = base_df[\"clean_text\"].apply(all_caps)\n","\n","# Define function to count capitalized words\n","def cap_words(text):\n","    if str(text) == \"\":\n","        return 0\n","    else:\n","        caps_list = list(map(str.capitalize, str(text).split()))\n","        count = 0\n","        for w in list(set(caps_list)):\n","            if w.strip(custom_punctuation1).isalpha():\n","                count = count + (\" \" + text + \" \").count(\" \" + w + \" \")\n","        return count\n","\n","\n","base_df[\"cap_words\"] = base_df[\"clean_text\"].apply(cap_words)\n","\n","# Basic Sentiment Score\n","analyzer = SentimentIntensityAnalyzer()\n","\n","\n","def compound(text):\n","    return analyzer.polarity_scores(text)[\"compound\"]\n","\n","\n","base_df[\"sentiment_compound\"] = base_df[\"no_url_text\"].apply(compound)\n","\n","\n","def neg(text):\n","    return analyzer.polarity_scores(text)[\"neg\"]\n","\n","\n","base_df[\"sentiment_neg\"] = base_df[\"no_url_text\"].apply(neg)\n","\n","\n","def pos(text):\n","    return analyzer.polarity_scores(text)[\"pos\"]\n","\n","\n","base_df[\"sentiment_pos\"] = base_df[\"no_url_text\"].apply(pos)\n","\n","\n","def neu(text):\n","    return analyzer.polarity_scores(text)[\"neu\"]\n","\n","\n","base_df[\"sentiment_neu\"] = base_df[\"no_url_text\"].apply(neu)\n","\n","# Drop tweets with no text or url data\n","ml_df = base_df[base_df[\"text\"].notna() | base_df[\"urls\"].notna()]\n","dropped = base_df[base_df[\"text\"].isna() & base_df[\"urls\"].isna()]\n","\n","# df for base model\n","simple_df = base_df[base_df[\"text\"].notna()]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Let's try spacy\n","import en_core_web_sm\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from spacy.lang.en import English\n","\n","from sklearn.base import TransformerMixin\n","from sklearn.pipeline import Pipeline\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","\n","nlp = en_core_web_sm.load()\n","\n","# Punctuation marks\n","punctuations = string.punctuation\n","\n","# Add nan to STOP_WORDS\n","# STOP_WORDS\n","\n","# Load English tokenizer, tagger, parser, NER and word vectors\n","parser = English()\n","\n","# Creating our tokenizer function\n","def spacy_tokenizer1(sentence):\n","    # Creating our token object, which is used to create documents with linguistic annotations.\n","    mytokens = parser(sentence)\n","\n","    # Lemmatizing each token and converting each token into lowercase\n","    mytokens = [\n","        word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_\n","        for word in mytokens\n","    ]\n","\n","    # Removing stop words\n","    mytokens = [\n","        word for word in mytokens if word not in STOP_WORDS and word not in punctuations\n","    ]\n","\n","    # return preprocessed list of tokens\n","    return mytokens\n","\n","\n","# Custom transformer using spaCy\n","class predictors(TransformerMixin):\n","    def transform(self, X, **transform_params):\n","        # Cleaning Text\n","        return [clean_text(text) for text in X]\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","\n","    def get_params(self, deep=True):\n","        return {}\n","\n","\n","# Basic function to clean the text\n","def clean_text(text):\n","    return text.strip().lower()\n","\n","\n","# Create spacy vectorizers\n","spacy_cv1 = CountVectorizer(tokenizer=spacy_tokenizer1, ngram_range=(1, 1))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_classifier = MultinomialNB()\n","\n","# Make train test splits\n","x = ml_df[\"clean_text\"]\n","y = ml_df[\"party\"]\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n","\n","# Create pipeline\n","pipe = Pipeline(\n","    [\n","        (\"cleaner\", predictors()),\n","        (\"vectorizer\", spacy_cv1),\n","        (\"classifier\", test_classifier),\n","    ]\n",")\n","\n","# model generation\n","pipe.fit(x_train, y_train)\n","\n","# Predicting with a test dataset\n","predicted = pipe.predict(x_test)\n","\n","# Model Accuracy\n","print(\"Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.tokenize import TweetTokenizer\n","from nltk.tokenize import RegexpTokenizer\n","\n","nltk_tk_tweet = TweetTokenizer().tokenize\n","nltk_tk_regexp = RegexpTokenizer(r\"[a-zA-Z0-9]+\").tokenize\n","\n","custom_punctuation2 = punctuations.replace(\"@\", \"\").replace(\"!\", \"\").replace(\"#\", \"\")\n","\n","\n","# text_counts = spacy_cv1.fit_transform(ml_df[\"clean_text\"])\n","# feature_names = spacy_cv1.get_feature_names()\n","\n","custom_nlp = en_core_web_sm.load()\n","\n","\n","def spacy_tokenizer2(sentence):\n","    # Creating our token object, which is used to create documents with linguistic annotations.\n","    spacy_nlp = custom_nlp(sentence)\n","\n","    # Lemmatizing each token and converting each token into lowercase\n","    mytokens = [\n","        word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_\n","        for word in spacy_nlp\n","    ]\n","\n","    # Removing stop words\n","    mytokens = [\n","        word for word in mytokens if word not in STOP_WORDS and word not in punctuations\n","    ]\n","\n","    # return preprocessed list of tokens\n","    return mytokens\n","\n","\n","sentence = \"Donald Trump visited Apple headquarters THIS IS a smaple as;odfi n soid lkloiw fj;aklsjfd -sdiof @cnn #jfs apple.com www.foxnews.com\"\n","sentence2 = \"THis is a test sentance to see how SPACY works White Collar espn.com\"\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","\n","ngrams = [(1, 1), (1, 2)]\n","tokenizer_list = [[\"NTLK\", nltk_tk_regexp], [\"SPACY\", spacy_tokenizer1]]\n","model_list = [MultinomialNB(), RandomForestClassifier(), LogisticRegression()]\n","rstates = [i for i in range(1, 3)]\n","x_data = [[\"Clean\", ml_df[\"clean_text\"]], [\"Dirty\", ml_df[\"text\"]]]\n","vector_methods = [\"Count\", \"TFIDF\"]\n","\n","results_list = []\n","total_len = (\n","    len(ngrams)\n","    * len(tokenizer_list)\n","    * len(model_list)\n","    * len(rstates)\n","    * len(x_data)\n","    * len(vector_methods)\n",")\n","y = ml_df[\"party\"]\n","tsize = 0.3\n","counter = 0\n","\n","t0 = time.time()\n","\n","for dataset in x_data:\n","    x = dataset[1]\n","    for t in tokenizer_list:\n","        tokenizer = t[1]\n","        for v in vector_methods:\n","            vector_type = v  # Specify count, if not count defaults to TFIDF\n","            for ng in ngrams:\n","                ngram = ng\n","\n","                # Specify vectorizer\n","                if vector_type == \"Count\":\n","                    vect = CountVectorizer(\n","                        tokenizer=tokenizer, lowercase=True, ngram_range=ngram\n","                    )\n","                else:\n","                    vect = TfidfVectorizer(tokenizer=tokenizer, ngram_range=ngram)\n","\n","                text_vect = vect.fit_transform(x)\n","\n","                for r in rstates:\n","                    rstate = r\n","\n","                    # Make new train test splits\n","                    x_train, x_test, y_train, y_test = train_test_split(\n","                        text_vect, y, test_size=tsize, random_state=rstate\n","                    )\n","                    for c in model_list:\n","                        classifier = c\n","\n","                        model = classifier.fit(x_train, y_train)\n","                        prediction = model.predict(x_test)\n","\n","                        null_prediction = y_test.replace(\n","                            to_replace=ml_df[\"party\"]\n","                            .value_counts()\n","                            .sort_values()\n","                            .index[0],\n","                            value=ml_df[\"party\"].value_counts().sort_values().index[1],\n","                        )\n","\n","                        accuracy = metrics.accuracy_score(y_test, prediction)\n","                        dem_precision = metrics.precision_score(\n","                            y_test, prediction, pos_label=\"Democrat/Ind\"\n","                        )\n","                        rep_precision = metrics.precision_score(\n","                            y_test, prediction, pos_label=\"Republican\"\n","                        )\n","                        dem_recall = metrics.recall_score(\n","                            y_test, prediction, pos_label=\"Democrat/Ind\"\n","                        )\n","                        rep_recall = metrics.recall_score(\n","                            y_test, prediction, pos_label=\"Republican\"\n","                        )\n","                        null_accuracy = metrics.accuracy_score(y_test, null_prediction)\n","                        lift = accuracy / null_accuracy\n","\n","                        results_list.append(\n","                            [\n","                                str(classifier).split(\"(\")[0],\n","                                t[0],\n","                                vector_type,\n","                                str(ngram),\n","                                rstate,\n","                                dataset[0],\n","                                accuracy,\n","                                dem_precision,\n","                                rep_precision,\n","                                dem_recall,\n","                                rep_recall,\n","                                null_accuracy,\n","                                lift,\n","                            ]\n","                        )\n","                        counter = counter + 1\n","                        print(\n","                            \"Completed \"\n","                            + str(counter)\n","                            + \"/\"\n","                            + str(total_len)\n","                            + \" models.\"\n","                        )\n","t2 = time.time()\n","print(\"Completed all models in \" + str(round((t2 - t1), 2)) + \" seconds.\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import TruncatedSVD\n","\n","n_comps = 50\n","\n","svd = TruncatedSVD(n_components=n_comps)\n","components = svd.fit_transform(text_vect)\n","print(\"Explained variance from SVD:\" + str(sum(svd.explained_variance_ratio_)))\n","\n","components_df = pd.DataFrame(components).reset_index(drop=True)\n","components_df.columns = [\"SVD\" + str(i + 1) for i in range(n_comps)]\n","\n","# Meta-data from initial analysis\n","md_df = ml_df[\n","    [\n","        \"num_days\",\n","        \"all_caps\",\n","        \"cap_words\",\n","        \"sentiment_compound\",\n","        \"sentiment_pos\",\n","        \"sentiment_neu\",\n","        \"sentiment_neg\",\n","    ]\n","].reset_index(drop=True)\n","for n in list(md_df.columns):\n","    components_df[n] = md_df[n]\n","\n","# Specify post-SVD parameters\n","svd_classifier = RandomForestClassifier()\n","svd_tsize = 0.3\n","svd_rstate = 1\n","svd_x = components_df\n","svd_y = ml_df[\"party\"]\n","\n","\n","svd_t0 = time.time()\n","\n","# Make new train test splits\n","svd_x_train, svd_x_test, svd_y_train, svd_y_test = train_test_split(\n","    svd_x, svd_y, test_size=svd_tsize, random_state=svd_rstate\n",")\n","\n","svd_model = svd_classifier.fit(svd_x_train, svd_y_train)\n","svd_prediction = svd_model.predict(svd_x_test)\n","\n","svd_null_prediction = svd_y_test.replace(\n","    to_replace=svd_y.value_counts().sort_values().index[0],\n","    value=svd_y.value_counts().sort_values().index[1],\n",")\n","\n","print(\n","    str(svd_classifier).split(\"(\")[0] + \" SVD Accuracy:\",\n","    metrics.accuracy_score(svd_y_test, svd_prediction),\n",")\n","print(\n","    \"Null Accuracy:\", metrics.accuracy_score(svd_y_test, svd_null_prediction),\n",")\n","svd_t1 = time.time()\n","print(\"Model Time:\" + str(t1 - t0))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}